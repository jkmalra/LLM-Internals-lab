PHASE 1:
- autograd class deeply to understand
- Re-implement Value class
- Trace one forward backward pass manually

PHASE 2:
- Break down attention step-by-step
- Visualize attention weights
- Experiment with removing RMSNorm

PHASE 3:
- Rewrite minimal GPT in Python