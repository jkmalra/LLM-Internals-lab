# LLM-Internals-lab
Structured deep dive into transformer architecture and language model internals. Currently storing reference implementation (microgpt) for later breakdown and re-implementation. Planned exploration includes:  Autograd from scratch  Attention math breakdown  Transformer architecture analysis  Training loop mechanics  Scaling insights
