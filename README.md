# LLM-Internals-lab
This repository serve as structure into transformer architecture and language model internals.

Currently storing reference implementation (microgpt) for breakdown and re-implementation.


## What actually we are doing

1. Downloads a dataset of names

2. Builds a tokenizer (character-level)

3. Implements its own automatic differentiation engine

4. Implements a mini GPT (transformer) model

5. Trains it using backpropagation

6. Generates new fake names

> its complete LLM training in Loop